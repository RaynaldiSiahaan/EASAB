{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image\n",
    "\n",
    "f = r'E://Tugas AB/img'  # contoh nama direktori untuk naruh file image\n",
    "fo= r'E://Tugas AB/imgresize'\n",
    "\n",
    "for file in os.listdir(f):\n",
    "    f_img = f+\"/\"+file\n",
    "    fo_img = fo+\"/\"+file\n",
    "    print (f_img, '->', fo_img)\n",
    "    img = Image.open(f_img)\n",
    "    img = img.resize((100,100))\n",
    "    img.save(fo_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image\n",
    "\n",
    "def get_pca(filex, fileo):\n",
    "    # load data\n",
    "    img = cv2.imread(filex) \n",
    "    # pada contoh ini semua diubah menjadi gray level\n",
    "    # selain gray level, bisa bagian red, green, atau blue\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # initialize PCA, semakin sedikit jumlah PCA maka semakin \"buram\" hasil karena \n",
    "    # data hanya sebagian saja \n",
    "    pca = PCA(10)\n",
    " \n",
    "    #Applying to the channel \n",
    "    gray_pca = pca.fit_transform(gray)\n",
    "    \n",
    "    # simpan hasilnya ke csv\n",
    "    print (filex, '->', fileo, '->', gray_pca.shape)\n",
    "    # print (gray_pca.shape)\n",
    "    # pd.DataFrame(np_array).to_csv(\"path/to/file.csv\")\n",
    "    np.savetxt(fileo, gray_pca, delimiter=\",\")\n",
    "    return \n",
    "\n",
    "f = r'E:\\Tugas AB\\imgresize'             # direktori image asal\n",
    "fo= r'E:\\Tugas AB\\csv'       # direktori output\n",
    "\n",
    "for file in os.listdir(f):\n",
    "    f_img = f+\"/\"+file\n",
    "    fo_img = fo+\"/\"+file\n",
    "    fo_img_x = fo_img.replace(\".\", \"-\" )\n",
    "    fo_img_x = fo_img_x+\".csv\"\n",
    "    get_pca(f_img, fo_img_x)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "filex = \"ironman.jpg\"\n",
    "# load data\n",
    "img = cv2.imread(filex) \n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# initialize PCA, semakin sedikit jumlah PCA maka semakin \"buram\" hasil karena \n",
    "# data hanya sebagian saja \n",
    "pca = PCA(10)\n",
    " \n",
    "#Applying to the channel \n",
    "gray_pca = pca.fit_transform(gray)\n",
    "gray_inverted = pca.inverse_transform(gray_pca)\n",
    "plt.imshow(gray)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(gray_inverted)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.indexes.range import RangeIndex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# set config used for PC and width photos pixel\n",
    "# jangan lupa, jumlah PC agar disesuaikan apabila diganti\n",
    "PC = 10\n",
    "width = 100         # jika ukuran image adalah 100 x 100\n",
    "formatPrint = \"PC{col}.{row}\"\n",
    "columnName = ['label']\n",
    "\n",
    "for i in range(PC):\n",
    "    for j in range(width):\n",
    "        columnName.append(formatPrint.format(col = i+1, row = j+1))\n",
    "\n",
    "columnName.append('class')\n",
    "dataframe = pd.DataFrame(columns=columnName)\n",
    "directory = 'csv/'\n",
    "\n",
    "print (dataframe)\n",
    "\n",
    "for filename in os.listdir('csv/'):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        nmfile = directory + filename        \n",
    "        df = pd.read_csv(nmfile, header=None, float_precision='round_trip')\n",
    "        # print (df)\n",
    "        array = df.transpose().to_numpy().flatten()\n",
    "        label = os.path.splitext(filename)[0]\n",
    "        data = [label]\n",
    "        dd = array.tolist()\n",
    "        data = data + dd + data\n",
    "        print(nmfile, '->', len(dataframe), '->', len(columnName), '->', len(data))\n",
    "        dataframe.loc[len(dataframe)] = data\n",
    "\n",
    "dataframe.to_excel('dataset-new4.xlsx')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataframe = pandas.read_csv(\"E:\\Tugas AB\\dataset-new4.csv\")\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:1001].astype(float)\n",
    "y = dataset[:,1001]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#-----------\n",
    "# create model  \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "clf = model.fit(X_train, y_train)\n",
    "#------------\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Use score method to get accuracy of the model\n",
    "#score_te = model.score(X_test, y_test)\n",
    "#print('Accuracy Score: ', score_te)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (cm)\n",
    "\n",
    "# Use accuracy_score to get accuracy of the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy Score: ', acc)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "# Assume you have the XYZ dataset in the variables X and y\n",
    "# X contains the features (XYZ values)\n",
    "# y contains the corresponding class labels\n",
    "dataframe = pandas.read_csv(\"E:\\Tugas AB\\dataset-new4.csv\")\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:1001].astype(float)\n",
    "y = dataset[:,1001]\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # Set the number of neighbors (k) as 3\n",
    "\n",
    "# Step 4: Train the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the classifier\n",
    "\n",
    "# Step 6: Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "jumlahclass = 7\n",
    "\n",
    "directory = r'E:\\Tugas AB\\imgresize'  # alamat lengkap\n",
    "\n",
    "images = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.jpg'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        image = Image.open(file_path)\n",
    "        images.append(np.array(image))\n",
    "# for image in images:\n",
    "#     print(image.mode)\n",
    "\n",
    "myimages = np.array(images)\n",
    "print(myimages.shape)\n",
    "\n",
    "# agar mudah\n",
    "\n",
    "dummy = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "         1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "         2,2,2,2,2,2,2,2,2,2,2,2,2,2,\n",
    "         3,3,3,3,3,3,3,3,3,3,3,3,3,3,\n",
    "         4,4,4,4,4,4,4,4,4,4,4,4,4,4,\n",
    "         5,5,5,5,5,5,5,5,5,5,5,5,5,5,\n",
    "         6,6,6,6,6,6,6,6,6,6,6,6,6,6 ]\n",
    "\n",
    "X = myimages\n",
    "y = np.array(dummy)\n",
    "\n",
    "# Shuffle the dataset\n",
    "random_state = 42  # Set a random seed for reproducibility\n",
    "#X_shuffled, y_shuffled = np.random.shuffle(X, y, random_state=random_state)\n",
    "\n",
    "# Split the shuffled dataset into training and testing sets\n",
    "test_size = 0.2  # Specify the percentage of the dataset to be used for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "print (\"======== setelah diacak\")\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1, 100, 100, 3).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 100, 100, 3).astype('float32') / 255.0\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(100, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(100, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(100, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(jumlahclass, activation='softmax')) # Sesuaikan dengan class\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'E:\\Tugas AB\\Raip.jpg'\n",
    "image = Image.open(image_path).resize((100, 100))  # Resize the image\n",
    "image = np.array(image)  # Convert image to a numpy array\n",
    "image = image.astype('float32') / 255.0  # Normalize the pixel values\n",
    "image = np.expand_dims(image, axis=0)  # Add an extra dimension for batch size\n",
    "\n",
    "predictions = model.predict(image)\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "class_labels = ['Ega', 'Irfan', 'Nopal', 'Pak', 'Peter', 'Raynaldi', 'Yogik']  # Replace with your class labels\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "print(f'Predicted class: {predicted_class_label}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
